{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "#Define file paths\n",
    "big_data_file = '/home/ubuntu/myraidstorage/cryptobot_cs554_project/big-btc-sentiment-time-series.csv'\n",
    "btcusd_file = '/home/ubuntu/myraidstorage/cryptobot_cs554_project/btcusd-2018-2023-copy.csv'\n",
    "jan18_sentiment = '/home/ubuntu/myraidstorage/cryptobot_cs554_project/RS_2018-01-cleaned_updated_sentiment_analysis_new.csv'\n",
    "feb18_sentiment = '/home/ubuntu/myraidstorage/cryptobot_cs554_project/RS_2018-02-cleaned_updated_sentiment_analysis_new.csv'\n",
    "march18_sentiment = '/home/ubuntu/myraidstorage/cryptobot_cs554_project/RS_2018-03-cleaned_updated_sentiment_analysis_new.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- Step 1: Create model_data.csv and write header row-----------------#\n",
    "with open(big_data_file, mode='w', newline='') as model_file:\n",
    "    writer = csv.writer(model_file)\n",
    "    writer.writerow(['date', 'reddit_sentiment_aggregate', 'positive_post_volume', 'negative_posts_volume',\n",
    "                     'positive_sentiment_volume_change', 'negative_sentiment_volume_change', 'BTC_trade_volume',\n",
    "                     'BTC_closing_price', 'BTC_price_change', 'SP500_price', 'SP500_price_change', 'crude_oil_price',\n",
    "                     'crude_oil_price_change'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m         model_rows[i][\u001b[39m8\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mfloat\u001b[39m(model_rows[i][\u001b[39m7\u001b[39m]) \u001b[39m-\u001b[39m \u001b[39mfloat\u001b[39m(model_rows[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m7\u001b[39m]))  \u001b[39m# BTC price change\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m#test\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mprint\u001b[39m(model_rows[\u001b[39m3\u001b[39;49m][\u001b[39m7\u001b[39m])\n\u001b[1;32m     23\u001b[0m \u001b[39m# write updated rows to model_data.csv\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(big_data_file, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, newline\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m outfile:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#-----------------Step 2: Add BTC data from btcusd-2018-2023.csv for days 1-Jan-18 through 31-March-18-----------------#\n",
    "with open(btcusd_file) as btc_file, \\\n",
    "        open(big_data_file, mode='r', newline='') as model_file:\n",
    "    reader = csv.reader(btc_file)\n",
    "    next(reader)  # skip header row\n",
    "    next(model_file)  # skip header row\n",
    "    model_reader = csv.reader(model_file)\n",
    "    model_rows = list(model_reader)\n",
    "    btc_rows = list(reader)\n",
    "    j = 0\n",
    "    for i in range(len(model_rows)):\n",
    "        model_date = datetime.strptime(model_rows[i][0], '%d-%b-%y')\n",
    "        btc_date = datetime.strptime(btc_rows[j][0], '%Y-%m-%d %H:%M:%S')\n",
    "        while btc_date < model_date:\n",
    "            j += 1\n",
    "            btc_date = datetime.strptime(btc_rows[j][0], '%Y-%m-%d %H:%M:%S')\n",
    "        model_rows[i][7] = btc_rows[j][4]  # BTC closing price\n",
    "        if i > 0:\n",
    "            model_rows[i][8] = str(float(model_rows[i][7]) - float(model_rows[i-1][7]))  # BTC price change\n",
    "\n",
    "   \n",
    "    # write updated rows to model_data.csv\n",
    "    with open(big_data_file, mode='a', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerows(model_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------------- Step 3: Fill in SP500 data -----------------#\n",
    "with open('sp500.csv') as sp500_file, \\\n",
    "        open('model_data.csv', mode='r', newline='') as model_file:\n",
    "    reader = csv.reader(sp500_file)\n",
    "    next(reader)  # skip header row\n",
    "    next(model_file)  # skip header row\n",
    "    model_reader = csv.reader(model_file)\n",
    "    model_rows = list(model_reader)\n",
    "    sp500_rows = list(reader)\n",
    "    for i in range(len(model_rows)):\n",
    "        model_date = datetime.strptime(model_rows[i][0], '%m/%d/%Y').strftime('%Y-%m-%d')\n",
    "        for j in range(len(sp500_rows)):\n",
    "            sp500_date = datetime.strptime(sp500_rows[j][0], '%Y-%m-%d').strftime('%m/%d/%Y')\n",
    "            if sp500_date == model_date:\n",
    "                model_rows[i][9] = sp500_rows[j][1]\n",
    "                break\n",
    "    with open('model_data.csv', mode='w', newline='') as model_file:\n",
    "        writer = csv.writer(model_file)\n",
    "        writer.writerows(model_rows)\n",
    "        \n",
    "    # Calculate SP500 price change\n",
    "    for i in range(len(model_rows)):\n",
    "        if i == 0:\n",
    "            model_rows[i].append(None)\n",
    "        else:\n",
    "            prev_price = float(model_rows[i-1][9])\n",
    "            curr_price = float(model_rows[i][9])\n",
    "            model_rows[i].append(curr_price - prev_price)\n",
    "    with open('model_data.csv', mode='w', newline='') as model_file:\n",
    "        writer = csv.writer(model_file)\n",
    "        writer.writerows(model_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-----------------Step 4-----------------#\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "with open('crude_oil_price.csv') as crude_oil_file, \\\n",
    "        open('model_data.csv', mode='r', newline='') as model_file:\n",
    "    reader = csv.reader(crude_oil_file)\n",
    "    next(reader)  # skip header row\n",
    "    next(model_file)  # skip header row\n",
    "    model_reader = csv.reader(model_file)\n",
    "    model_rows = list(model_reader)\n",
    "    crude_oil_rows = list(reader)\n",
    "    for i in range(len(model_rows)):\n",
    "        model_date = datetime.strptime(model_rows[i][0], '%Y-%m-%d').date()\n",
    "        crude_oil_date = datetime.strptime(crude_oil_rows[i][0], '%m/%d/%Y').date()\n",
    "        while crude_oil_date < model_date:\n",
    "            # move to the next row in the crude oil data until we reach a date that's equal to or later than the model date\n",
    "            crude_oil_rows.pop(0)\n",
    "            crude_oil_date = datetime.strptime(crude_oil_rows[0][0], '%m/%d/%Y').date()\n",
    "        if crude_oil_date == model_date:\n",
    "            # if we found the matching date, fill in the crude oil price and crude oil price change columns\n",
    "            model_rows[i][10] = crude_oil_rows[0][1]\n",
    "            if i > 0:\n",
    "                prev_crude_oil_price = float(crude_oil_rows[0][1])\n",
    "                curr_crude_oil_price = float(crude_oil_rows[1][1])\n",
    "                crude_oil_price_change = (curr_crude_oil_price - prev_crude_oil_price) / prev_crude_oil_price\n",
    "                model_rows[i][11] = crude_oil_price_change\n",
    "            crude_oil_rows.pop(0)\n",
    "        else:\n",
    "            # if we didn't find the matching date, print a warning message and leave the crude oil price and crude oil price change columns blank\n",
    "            print(f'Warning: No crude oil price data for {model_date}')\n",
    "    with open('model_data.csv', mode='w', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(['date', 'reddit_sentiment_aggregate', 'positive_post_volume', 'negative_posts_volume',\n",
    "                         'positive_sentiment_volume_change', 'negative_sentiment_volume_change', 'BTC_trade_volume',\n",
    "                         'BTC_closing_price', 'BTC_price_change', 'SP500_price', 'SP500_price_change', 'crude_oil_price',\n",
    "                         'crude_oil_price_change'])\n",
    "        for row in model_rows:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'timestamp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# # Calculate daily sentiment aggregate\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# sentiment_df['sentiment_value'] = sentiment_df['predicted_category'].map({'bearish': -1, 'bullish': 1, 'neutral': 0})\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# sentiment_df['weighted_sentiment_value'] = sentiment_df['sentiment_value'] * sentiment_df['confidence']\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[39m# Aggregate sentiment to daily\u001b[39;00m\n\u001b[1;32m     17\u001b[0m daily_sentiment[\u001b[39m'\u001b[39m\u001b[39mreddit_sentiment_aggregate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m sentiment_df\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: np\u001b[39m.\u001b[39maverage(x[\u001b[39m'\u001b[39m\u001b[39mpredicted_category\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap({\u001b[39m'\u001b[39m\u001b[39mbearish\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbullish\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mneutral\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m}), weights\u001b[39m=\u001b[39mx[\u001b[39m'\u001b[39m\u001b[39mconfidence\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m---> 18\u001b[0m daily_sentiment[\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(daily_sentiment[\u001b[39m'\u001b[39;49m\u001b[39mtimestamp\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     19\u001b[0m daily_sentiment\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m \u001b[39m# Calculate post volume\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'timestamp'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#-----------------Step 5-----------------#\n",
    "# Load sentiment data\n",
    "sentiment_files = [jan18_sentiment, feb18_sentiment, march18_sentiment]\n",
    "sentiment_dfs = []\n",
    "for sentiment_file in sentiment_files:\n",
    "    df = pd.read_csv(sentiment_file)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s').dt.date\n",
    "    sentiment_dfs.append(df)\n",
    "sentiment_df = pd.concat(sentiment_dfs, ignore_index=True)\n",
    "\n",
    "# # Calculate daily sentiment aggregate\n",
    "# sentiment_df['sentiment_value'] = sentiment_df['predicted_category'].map({'bearish': -1, 'bullish': 1, 'neutral': 0})\n",
    "# sentiment_df['weighted_sentiment_value'] = sentiment_df['sentiment_value'] * sentiment_df['confidence']\n",
    "\n",
    "# Aggregate sentiment to daily\n",
    "daily_sentiment['reddit_sentiment_aggregate'] = sentiment_df.groupby('timestamp').apply(lambda x: np.average(x['predicted_category'].map({'bearish': -1, 'bullish': 1, 'neutral': 0}), weights=x['confidence']))\n",
    "daily_sentiment['timestamp'] = pd.to_datetime(daily_sentiment['timestamp'])\n",
    "daily_sentiment.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Calculate post volume\n",
    "positive_post_volume = sentiment_df[sentiment_df['predicted_category'] == 'bullish'].groupby('timestamp').size().rename('positive_post_volume')\n",
    "negative_post_volume = sentiment_df[sentiment_df['predicted_category'] == 'bearish'].groupby('timestamp').size().rename('negative_post_volume')\n",
    "daily_sentiment = daily_sentiment.join([positive_post_volume, negative_post_volume], how='outer')\n",
    "\n",
    "# Fill in missing days\n",
    "start_date = sentiment_df['timestamp'].min()\n",
    "end_date = sentiment_df['timestamp'].max()\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "daily_sentiment = daily_sentiment.reindex(date_range, fill_value=0)\n",
    "\n",
    "# Save data to CSV\n",
    "daily_sentiment.to_csv(big_data_file, index_label='date')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
